
\documentclass[fleqn,12pt]{article}

\usepackage[margin=15mm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[bulgarian]{babel}
\usepackage[unicode]{hyperref}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{enumitem, hyperref}
\usepackage{upgreek}
\usepackage{indentfirst}
\usepackage{array}
\usepackage{listings}

\usepackage{amsmath}
\DeclareMathOperator{\cotg}{cotg}
\DeclareMathOperator{\LCS}{LCS}
\DeclareMathOperator{\longer}{longer}

\title{Структури от данни и алгоритми. Анализ на алгоритми. Абстрактни
типове от данни. Стек, опашка, списък, дърво. Сортиране.}

\author{v0.1}
\date{24 юни 2021}

\begin{document}

\maketitle

\tableofcontents

\section{Анализ на алгоритми}
\subsection{Въведение / необходимост}
TODO

\subsection{Асимптотична нотация на сложността}
\subsubsection{Дефиниция}
Нека $f, g : \mathbb{N} \rightarrow \mathbb{R}$. Казваме, че $f$ расте по-бавно от $g$ (или че $g$ мажорира $f$) и означаваме $f \prec g$, ако 
съществува $c \in \mathbb{R}$ и $n_0 \in \mathbb{N}$, за които е изпълнено $N > n_0 \Rightarrow f(N) < c.g(N)$.
Чрез $f \preceq g$ означаваме нестрогата форма на неравенството - $g$ расте поне толкова бързо, колкото $f$.

Нека $f : \mathbb{N} \rightarrow \mathbb{R}$. Дефинираме следните множества чрез релациите $\prec$ и $\preceq$:
\begin{itemize}
    \item $O(f) = \{g \hspace{2mm} | \hspace{2mm} g \preceq f\}$
    \item $o(f) = \{g \hspace{2mm} | \hspace{2mm} g \prec f\}$
    \item $\Omega(f) = \{g \hspace{2mm} | \hspace{2mm} f \preceq g\}$
    \item $\omega(f) = \{g \hspace{2mm} | \hspace{2mm} f \prec g\}$
    \item $\Theta(f) = \{g \hspace{2mm} | \hspace{2mm} f \preceq g \preceq f\}$
\end{itemize}

\subsubsection{Наредба на стандартните функции}
\noindent\[ C \prec \log \log N \prec \log N \prec \sqrt{N} \prec N \prec N^2 \prec N^3 \prec 2^N \prec 3^N \prec N! \prec N^N \]

\subsubsection{Свойства и примери}
Ако $f \preceq g$ и $h(n) = f(n) + g(n)$, то $h(n) \in \Theta(g)$.
Следвателно ни интересува само най-големия член на дадена формула, например
$N^2 + \log N + 5 \in O(N^2)$.

\subsection{Основни рекурентни формули}
\subsubsection{Дефиниция}
Нека с $T(N)$ обозначим времето за работа на алгоритъм при размер на входа $N$.
Ако можем да изразим $T(N)$ чрез формула, съдържаща $T$, то изразът наричаме \textbf{рекуретна формула}.
Такива изрази възникват често, когато алгоритми използват рекурсия.

Рекурентните формули можем да разглеждаме и като \textbf{рекурентни уравнения}, в които целта е да намерим израз за $T(N)$,
който не е рекурентна формула. Рекурентни уравнения могат да се решат по няколко начина: с индукция, с характеристично уравнение,
с мастър теорема и т.н.

Времето $T(N)$ можем да разглеждаме в най-лошия, средния и най-добрия случай (подредени по важност).
Освен време, по същия начин можем да разглеждаме и паметта, която алгоритъмът използва - нека я обозначим $M(N)$.
Очевидно $M(N) \preceq T(N)$.

\subsubsection{Примери}

\begin{center}
\begin{tabular}{|c|c|m{80mm}|}
    \hline
    Формула & Сложност & Примерен алгоритъм \\ 
    \hline
    $T(N) = T(N - 1) + N$ & $\Theta(N^2)$ & Наивна сортировка \\  
    \hline
    $T(N) = T\left(\dfrac{N}{2}\right) + 1$ & $\Theta(\log N)$ & Двоично търсене \\  
    \hline    
    $T(N) = T\left(\dfrac{N}{2}\right) + N$ & $\Theta(N)$ & Среден случай на търсене на медиана с \textbf{quick select} \\  
    \hline   
    $T(N) = 2T\left(\dfrac{N}{2}\right) + N$ & $\Theta(N \log N)$ & Бърза сортировка \\  
    \hline
    $T(N) = 2T\left(\dfrac{N}{2}\right) + 1$ & $\Theta(N)$ & Търсене на минимален елемент по схемата "разделяй и владей" \\  
    \hline     
\end{tabular}
\end{center}

\subsection{Примери за анализ на алгоритми}

\subsubsection{Двоично търсене}
Ще докажем, че сложността на двоичното търсене е $O(\log N)$.
Нека обозначим с $l_k$ и $r_k$ стойностите на променливите $left$ и $right$ на $k$-тата итерация на цикъла (нека първата итерация е $k = 0$).
Тогава размерът на масива, в който търсим, на всяка итерация е $N_k = r_k - l_k + 1$. За индекса на сравнение $m_k$ получваме
$m_k = \Big\lfloor\dfrac{l_k + r_k}{2}\Big\rfloor$ Интересува ни какво е $N_{k + 1}$, като имаме два случая:
\begin{itemize}
    \item $l_{k+1} = l_k, r_{k+1} = m_k - 1$. Тогава $N_{k+1} = \Big\lfloor\dfrac{l_k + r_k}{2}\Big\rfloor - 1 - l_k + 1 \leq \dfrac{l_k + r_k}{2} - l_k = \dfrac{N_k - 1}{2}$.
    Съответно $N_{k+1} \leq \Bigg\lfloor \dfrac{N_k}{2} \Bigg\rfloor$.
    \item $l_{k+1} = m_k + 1, r_{k+1} = r_k \Rightarrow N_{k+1} = r_k - \Big\lfloor\dfrac{l_k + r_k}{2}\Big\rfloor - 1 + 1 < \dfrac{r_k - l_k}{2} + 1 = \dfrac{N_k - 1}{2} + 1$. 
    $\dfrac{N_k - 1}{2} + 1 = \dfrac{N_k + 1}{2} = \Bigg\lceil \dfrac{N_k}{2} \Bigg\rceil$.
    Съответно получваме $N_{k+1} < \Bigg\lceil \dfrac{N_k}{2} \Bigg\rceil \Leftrightarrow N_{k+1} \leq \Bigg\lceil \dfrac{N_k}{2} \Bigg\rceil - 1 \Leftrightarrow $
    $N_{k+1} \leq \Bigg\lfloor \dfrac{N_k}{2} \Bigg\rfloor$
\end{itemize}

И двата случая получихме $N_{k+1} \leq \Bigg\lfloor \dfrac{N_k}{2} \Bigg\rfloor$.
Можем да приложим неравенството няколко пъти, както и да почнем от $N_0 = N$. 
Тогава получваме $N_k \leq \Bigg\lfloor \dfrac{N}{2^k} \Bigg\rfloor$. Алгоритъмът спира при $N_k < 1$,
следователно може да има най-много $\log_2 N$ стъпки $\Rightarrow O(\log N)$.

Алтернативно, можем да съставим рекурентна формула за времето в най-лошия случай - $T(n) = T\left(\frac{N}{2}\right) + 1$.
Вече споменахме, че за тази формула $T(n) \in \theta(\log N)$. 

Ако искаме да докажем и коректността на алгоритъма, то трябва да намерим и докажем инвариантата на цикъла.
В този случай това е следното твърдение: \textit{Елементът със стойност $target$ \textbf{не} се намира в подмасивите с индекси $0 \dots l_k - 1$ и $r_k + 1 \dots size - 1$}.
Доказването на инвариантите на цикъл обаче е извън обхвата на темата, така че ще го пропуснем.

\begin{lstlisting}[language=C++, caption=Двоично търсене]
int binary_search(const int * arr, const int size, const int target)
{
    int left = 0;
    int right = size - 1;

    while (left <= right)
    {
        const int m = (left + right) / 2;
        if (arr[m] == target) return m;
        if (arr[m] < target) left = m + 1;
        else right = m - 1;
    }

    return -1;
}
\end{lstlisting}
    
\subsubsection{Сортиране с вмъкване}
Анализираме сложността на алгоритъма сортиране с вмъкване в \ref{sort:insertion}.

\section{Абстрактни типове от данни}
\subsection{Дефиниция и идея}
Често при програмирането използваме отделни компоненти наготово, без да ни интересува как точно си вършат работата.
Казваме, че такива компоненти са \textbf{абстракции}. Абстракциите улесняват писането на софтуер, като не ни карат 
да мислим за ненужни детайли.

Абстрактен тип данни е тип данни, който единствено дефинира какви операции поддържа и какъв \textbf{underlying} тип данни ползва.

\subsection{Интерфейс и реализация}
Поддържаните операции от някой абстрактен тип се наричат негов \textbf{интерфейс}. Интерфейсът е това, което клиентския код 
вижда и ползва. \textbf{Реализацията} е конкретен начин, по който е написан даден интерфейс - и съответно, абстрактен тип.
Тя остава скрита от клиентския код. За един интерфейс може да има много реализации.

\section{Свързани списъци}
\subsection{Дефиниция и структура}
Свързаните списъци се състоят от \textbf{възли}. Всеки възел съдържа стойност от \textbf{underlying} тип, както 
и указатели към предходен и/или следващ възел. В зависимост от това дали всеки възел съдържа един или два указателя,
списъкът се нарича \textbf{едносвързан} или \textbf{двусвързан}. Самият списък съдържа указател към първия възел, 
а при двусвързаните - понякога и към последния.

\subsection{Обработка на списъци}
Свързаните списъци поддържат операциите \textbf{добавяне} и \textbf{изтриване} на възли.
Понеже можем да променя указателите във всеки възел, можем ефикасно да режем и залепяме части от свързани списъци. 

\section{Структура от данни стек}
\subsection{Дефиниция и структура}
Стекът се оприличава на купчина обекти, защото поддържа следните операции:
\begin{itemize}
    \item Добавяне на елемент най-отгоре
    \item Премахване на най-горен елемент
    \item Проверяване на стойността на най-горния елемент
\end{itemize}

По-тази причина се казва, че стекът е \textbf{Last In, First Out (LIFO)} структура.

\subsection{Реализация}
Има няколко начина стек да се реализира:
\begin{itemize}
    \item Чрез статичен масив. При тази реализация стекът има краен размер. Пази се единствено индексът на най-горния елемент. 
    \item Чрез свързан списък. Най-горният елемент е последния възел на списъка.
    \item Чрез динамичен масив - ако текущият размер не стига, се алокира нов, по-голям, и елементите се копират там.
\end{itemize}

\section{Структура от данни опашка}
\subsection{Дефиниция и структура}
Опашката, както името й подсказва, поддържа следните операции:
\begin{itemize}
    \item Добавяне на елемент най-отзад
    \item Премахване на първия елемент (след това втория става първи)
    \item Проверяване на стойността на първия елемент
\end{itemize}

Опашката се нарича още \textbf{First In, First Out} структура.

\subsection{Реализация}
Възможните реализации са като при стека, с малки разлики. Ако се използват масиви, е необходимо да се съхранвят два индекса - на първи и на последен елемент.

\section{Дървета}
\subsection{Дефиниция и структура}
Дърветата са \textbf{ненасочени ациклични графи}, което означава че те са нелинейни структури от данни. Не е задължително да имат връх, който е обозначен за \textbf{корен}, но
напрактика повечето дървета се съхраняват като \textbf{коренови дървета}. Връзките в дървото могат да бъдат представени по няколко начина:
\begin{itemize}
    \item Чрез функция на бащите - всеки връх знае кой му е предшественика
    \item Чрез функция на наследниците - всеки връх знае кои са му преки наследници
    \item Имплицитно, чрез индекси (виж \ref{trees:heap})
\end{itemize}

\subsection{Понятия в дървета}
Върхове, които нямат наследници, се наричат \textbf{листа}. Броя наследници на даден връх наричаме негова \textbf{разклоненост}.
Разклоненост на дървото наричаме максималната допустима разклоненост на връх. Дължината на единствения път от корена до някой връх
наричаме \textbf{дълбочина} на върха. Максималната дълбочина на връх наричаме дълбочина на дървото.

\subsection{Типове дървета}
Ще разгледяме някои по-популярни типове дървета.

\subsubsection{Двоични дървета}
Това са дървета, чиято максимална разклоненост е 2. Ако всеки връх, който не е листо, има точно 2 деца,
то дървото се нарича \textbf{пълно двоично дърво}. Ако разликата между минималната и максималната дълбочина на връх - нека бъде $d$ - е 0, 
то дървото наричаме \textbf{балансирано двоично дърво}.

\subsubsection{Двоични дървета за търсене}
Това са двоични дървета, в които правим разлика между \textbf{ляво} и \textbf{дясно} дете.
Тези дървета имат допълнително ограничение: $left \leq parent \leq right$. 
Това позволява в тях да се извършват операциите \textbf{добавяне}, \textbf{премахване} и \textbf{търсене} за средно време $O(\log N)$.

Съществуват и самобалансиращи се двоични дървета за търсене. Те поддържат и допълнително условие: $d_{max} = 1$. При тях всички операции
се случват за време $O(\log N)$ в най-лошия случай. Това ги прави подходящи за имплементация на абстрактните типове \textbf{множество} и \textbf{речник}.

\subsubsection{Пирамида (heap)}
\label{trees:heap}
Пирамидите са почти пълни и почти перфектно балансирани двоични дървета - позволено е да липсват върхове единствено на последното (най-дълбоко) ниво.
Това ограничение позволява те да се съхраняват в масива без използване на указатели. Ако приемем, че масивите започват от 0, то тогава 
връх с индекс $i$ има родител на индекс $\Bigg\lfloor \dfrac{i - 1}{2} \Bigg\rfloor$ и ляво и дясно дете на индекси $2i + 1$, $2i + 2$.

Пирамидите поставят и друго ограничение - слаба наредба между родител и деца. По-конкретно, родителят трябва да е по-голям или по-малък
от децата, но не се казва кое дете трябва да е по-голямо. Това означава, че пирамидите ефикасно поддържат добавяне и премахване на елементи - за $O(\log N)$,
както и намиране на \textbf{най-голям / най-малък елемент} - за $O(1)$ време.


\section{Сортиране}
\subsection{Дефиниция}
Нека е дадена редицата $a_0, a_1, \dots, a_n$ и тоталната наредба (релация) $\leq$.
Задачата за сортирането е да се намери биекция $s : [0, n] \rightarrow [0, n]$, такава че 
за всяко $i = 1 \dots n$ е изпълнено $a_{s(i - 1)} \leq a_{s(i)}$.
Напрактика често се интерсуваме единствено от редицата $a_{s(0)}, a_{s(1)}, \dots, a_{s(n)}$.

Казваме, че алгоритъм за сортиране е стабилен, ако запазва реда на еднаквите елементи.

\subsection{Елементарни методи за сортиране}
Ще разгледаме три наивни алгоритъма за сортиране. Всеки от тях има сложност в най-лошият случай $O(N^2)$ \textbf{сравнения} 
и \textbf{не} изисква допълнителна памет.

\subsubsection{Сортиране с пряка селекция (Selection sort)}
Идеята тук е всеки път да намираме минималния елемент и да го поставя на първа позиция на текущия масив.
След всяка итерация текущия масив се смалява с 1 отляво. Предимството на този алгоритъм е че е прост и от всички
извършва минимален брой \textbf{размени}. Следователно си струва да се обмисли, ако цената на едно преместване е
много по-голяма от цената на едно сравнение. Алгоритъмът не е стабилен.

\begin{lstlisting}[language=C++, caption=Selection sort]
void selection_sort(int * arr, const int size)
{
    for (int i = 0; i < size - 1; i++)
    {
        int min_idx = i;
        for (int j = i + 1; j < size; j++) 
            if (arr[j] < arr[min_idx]) min_idx = j;
        std::swap(arr[i], arr[min_idx]);
    }
}
\end{lstlisting}

\subsubsection{Сортиране с вмъкване (Insertion sort)}
\label{sort:insertion}
Познат още като \textbf{метода на картоиграча}. Инвариантата му е, че на $i$-та стъпка (броейки от 0), подмасивът 
с индекси $0 \dots i - 1$ е сортиран. Следователно на всяка стъпка се намира мястото на елемента $A[i]$. 
Предимството идва от факта, че това може да отнеме по-малко от $i$ стъпки, следователно при почти сортирани масиви
времето за работа е $O(N)$. Алгоритъмът е стабилен.

\begin{lstlisting}[language=C++, caption=Insertion sort]
void insertion_sort(int * arr, const int size)
{
    for (int i = 1; i < size; i++)
    {
        const int new_val = arr[i];
        int idx = i;
        while (idx > 0 && arr[idx - 1] > new_val)
        {
            arr[idx] = arr[idx - 1];
            idx--; 
        }
        arr[idx] = new_val;
    }
}
\end{lstlisting}

Ще докажем, че времето за работа в най-лошия случай е $\Theta(N^2)$, а в най-добрия - $\Theta(N)$.
Външният цикъл \textbf{for} винаги се изпълнява $size$ на брой стъпки, защото е по брояч.
Нека с $d_i$ обозначим броя итерации, извършвани от вътрешния цикъл \textbf{while} на $i$-тата итерация на външния цикъл.
Лесно се вижда, че $d_i$ е точно разстоянието от правилната позиция на елемента, който ще вмъкваме, до края на масива.

В най-добрият случай $d_i = 0$ - когато масива е вече сортиран. В най-лошият случай $d_i = i$ - ако новият елемент е най-малкия.
Общият брой операции е $\sum_{i = 1}^{N} (d_i + 1) = N + \sum_{i = 1}^{N} d_i$.
При $d_i = 0$ това е просто $N \Rightarrow \Theta(N)$ време. При $d_i = i$ 
получаваме $N + \sum_{i = 1}^N i$, което е от класа сложност $\Theta(N^2)$.

\subsubsection{Сортиране с метода на балончето (Bubble sort)}
Този метод няма предимства пред останалите. По-трудно се разбира и не е по-бърз, но все пак е успял 
да се набута в учебниците по програмиране.

\subsubsection{Използване на наивни сортировки}
Наивните сортировки се използват когато $N$ е \textit{малко}. Колко точно е това \textit{малко} зависи от конркетния случай и хардуер.
Често по-бързите сортировки ползват наивните сортировки като база на рекурсията, като най-често за целта се ползва \textbf{insertion sort}, 
понеже на практика е най-бърз и може да се възползва от частично наредени входнни данни.

\subsection{Сортиране - QuickSort}
Може да се докаже, че всеки сортиращ алгоритъм, базиран на сравнения, ще извърши $\Omega(N \log N)$ сравнения в най-лошия случай.
Следователно алгоритми с $O(N \log N)$ са оптимални. Ще разгледаме един такъв - \textbf{quick sort}, или бързо сортиране.

Идеята на псевдокод е следната:
\begin{enumerate}
    \item \label{qs:first} Избираме елемент $k$ от масива
    \item \label{qs:partiton} Разделяме масива на лява и дясна част, такава че всички елемент в лявата част са $\leq k$, а всички в дясната - $> k$.
    \item Сортираме рекурсивно лявата и дясната част (т.е. прилагаме стъпка \ref{qs:first} върху тях)
\end{enumerate}

Дъното на рекурсията е при размери 0 и 1. Стъпка \ref{qs:partiton} се нарича \textbf{partition} и винаги се изпълнява за $\Theta(N)$.
За съжаление, няма прост $O(1)$ алгоритъм за стъпка \ref{qs:first}, който да гарантира $O(N \log N)$ време в най-лошия случай - времето може да е $O(N^2)$
при патологични входове. На практика често се изпозлва средния или случаен елемент от масива.

Възможно е да изберем $k$ да е медианата на масива - това ще ни струва време $\Theta(N)$, но гарантира цялостно време $\Theta(N \log N)$.
Няма да обясняваме как работи точно алгоритъма, защото е извън обхвата на темата.

Дали алгоритъмът използва допълнителна памет и дали е стабилен зависи от версията на \textbf{partition}. Ако не ползваме допълнителна памет,
сортирането не е стабилно и обратно.

\begin{lstlisting}[language=C++, caption=QuickSort]
// Rearange arr so that all elements <= val are in the front
// Return the index of some element that is equal to val after rearanging
int partition(int * arr, const int size, const int val)
{
    // ...
}

void quick_sort(int * arr, const int size)
{
    if (size < 2) return;
    const int k = arr[size / 2];

    const int k_idx = partition(arr, size, k);
    quick_sort(arr, k_idx + 1);
    quick_sort(arr + k_idx + 1, size - k_idx - 1);
}
\end{lstlisting}

\end{document}
